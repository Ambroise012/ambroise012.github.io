---
title: "Paper - Nodalida - Pruning, Distillation and Adapters"
collection: publications
permalink: publications/paper
excerpt: >
    The increasing size of pre-trained language models (PLMs) has led to requires a better understanding of optimization methods. 
    In this study, we propose three approaches, structured pruning, distillation, and adapters which we apply to the Bidirectional Transformers for Language Understanding (BERT) model. Our goal is to assess the correlation and interdependence, or lack thereof, among these three methods. Our results focus on the GLUE task with benchmarks including model size, accuracy, rejected emission rate, and speedup. For instance, the combination of pruning, distillation, and adapters can reach 4x inference speedup with a size reduction of 85% and 200% emissions savings and an average accuracy of 50 instead of 85 approximately. Before each use, we must consider the performance-reduction-speed trade-off. In this work, we propose several scenarios depending on the desired outcome. For example, if we want to maintain performance and reduce the model by 65%, then we should opt for the combination of adapters based tuning + pruning with a sparsity ratio of 0.6. 
date: '2024-08-01'
venue:
paperurl: '../files/Ambroise__Efficiency_project.pdf'
citation: 
---

